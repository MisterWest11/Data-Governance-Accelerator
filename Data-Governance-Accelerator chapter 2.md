# Chapter 2

In this chapter, we will be:
  * Defining data and its relationship to information.

  * Exploring the role of data in the 21st century.

  * Moving from data to insights.

  * Discovering the impact of big data.

We will indulge in the challenges of managing data abundance in organizations. Unlike traditional resource constraints(budget, time), organizations struggle with an *abundance of data*. This creates challenges in managing, protecting and utilizng this data effectively.

The vast amount of data, also known as big data, comes from various sources like daily operations, products, customers, and partners.

Effective data governance and management are crucial to unlock the potential of big data and improve organizational performance.

**Big Data**

Data has a specific meaning and its really important to be clear on the definition, particularly as we start talking about information, knowledge and insights. But its even more important as we consider data in the context of governance and management. Data governance is not the same as data management.

People often take the meaning of "data" for granted and might have different interpretations. This can lead to misunderstandings when requesting or providing data.

Data is distinct from information, knowledge, and insights. A clear definition helps avoid confusion between these related concepts.

Precise understanding of "data" is crucial of data governance, a process concerned with managing and protecting data effectively.

**Why all the focus on data?**

Data refers to collections of digitally stored units, in other words, stuff that is kept on a computing device. These units represent something meaningful when processed for a human or a computer. 

Single units of data are traditionally referref to as *datum* and multiple units as data. However, the term data is often used in singular and plural contexts and in this course.

Prior to proccessing, data doesn't need to make sense individually or even in combination with other data. Data is also defined based on its captured format. Specifically at a high level, it falls into of the following categories:

  * Structured: Data that has been formatted to a set structure. Each data unit fits nicely into a table in a database. Its ready for analysis. e.g. first name, last name, phone number...

  * Unstructured: Data that stored in a native format must be processed to be used. Further work is required to enable analysis. e.g. email content and social media posts.

  * Semi-structured: Data that contains additional information to enable the native format to be searched and analyzed.

This basic action of data processing cannot be overstated, as it represents the core foundation of an industry that has ushered in our current period of rapid digital transformation. Today, the term data processing has been replaced with information technology (IT).

# Welcome to The Zettabyte Era

A zettabyte is a unit of digitally storage that measures an incredibly large amount of data. 

* Zettabyte Era: The rapid growth of data creation led to the use of zettabytes to quantify this massive data volume.

* A Huge Number: One zettabyte is equal to 1021 bytes or a 1 followed by 21 zeros.

* Rapid Data Growth: By 2020, 64 zettabytes of data were created globally, and this number is expected to keep growing exponentially.

* Comparison: To store one zettabyte, you would need one billion terabyte drives.

* Technical Breakdown: A zettabyte is made up of bytes, and each byte consists of 8 bits. A bit is the fundamental unit of data storage on a computer and can be either a 0 or 1 (binary system).

* Future Units: As data volume keeps increasing, even larger units like yottabyte and brontobyte might be needed.

![image](https://github.com/MisterWest11/Data-Governance-Accelerator/assets/152319557/f679327f-9a2c-4932-9dcc-86554e479121)

A zettabyte is an incredibly large number, equal to 1021 bytes (1 followed by 21 zeros).

By 2020, the world had already created 64 zettabytes of data, and this amount is expected to keep growing rapidly.

The term "datasphere" refers to all this data.To store one zettabyte, you'd need one billion terabyte drives.

The passage also briefly explains bits and bytes, the building blocks of digital data. Zettabytes are even larger units, and we might need new terms like yottabyte and brontobyte in the future.

![image](https://github.com/MisterWest11/Data-Governance-Accelerator/assets/152319557/a6c4c0ae-8b1b-4669-bdb2-8647f252ead7)

![image](https://github.com/MisterWest11/Data-Governance-Accelerator/assets/152319557/04d2fff3-31e0-4716-b777-b7c2ec7f3772)

Managing a small amount of data can have challenges, but managing data at scale is materially more challenging.

# From Data to Insight

Creating, collecting and storing data is a waste of time and money if its being done without a clear purposes or intent to use it in the future. Certain exceptions may be logical to collect data even  when we dont have a reason because it may have value at some point in the future, but this is only an exception. Generally, an organization is onboarding because its required for some purpose.

![image](https://github.com/MisterWest11/Data-Governance-Accelerator/assets/152319557/ce63fb83-15ae-402c-9c7f-f17611d29dcb)

Data is raw, unprocessed elements.

Information adds context to the data.

Knowledge incorporates additional information and understanding.

Wisdom applies reasoning and experience to knowledge, providing a deeper understanding.

Insight uses wisdom to make a decision or take action.

![image](https://github.com/MisterWest11/Data-Governance-Accelerator/assets/152319557/6d9e8cdd-6d4f-4e98-8c3f-917ee4c081c4)

![image](https://github.com/MisterWest11/Data-Governance-Accelerator/assets/152319557/a1784074-85ce-460d-a788-aa20882c7280)

![image](https://github.com/MisterWest11/Data-Governance-Accelerator/assets/152319557/47c7ca8b-0da6-408b-b9cb-caf7a8d6de3a)

every day, different organizations with access to the same data have different outcomes. While the best outcome can't be guaranteed no matter which processes, tools or skills are used, good practices such as the right level of data governance can absolutely lead to better results.

# The Role of Data in the 21st Century

Historically, data has always been valuable for understanding the world, making informed decisions, and solving problems.

Since the mid-20th century, the role of data has grown significantly due to:

  * Increased quantity of data with the rise of computer systems.

  * Improved data quality and wider accessibility, particularly after the internet's arrival in the mid-1990s.


Today, we generate massive amounts of high-quality data that is transforming various aspects of our lives, including:

  * Industries and cities (tools and capabilities)

  * Learning

  * Socializing

  * Entertainment

    
The immense value of data also creates new risks, such as cyberattacks with potentially devastating consequences.

The author argues that data might be the most valuable asset in the world, signifying a new era for data and its impact on our careers.

# Data-Driven Decision-Making

Data empowers better decision-making. Reading online reviews is a simple example of this concept in action.

Data-driven decisions can range from simple choices (like picking a restaurant) to complex strategic choices (like entering a new market).

Good quality data is crucial for effective decision-making. Using bad data can lead to poor choices and negative consequences.

The abundance of data we have today is a feature of the 21st century, but ensuring data quality requires specific actions and data governance plays a central role in achieving this.

# Data as The New Oil

* Data is like oil in the sense that it fuels the digital economies of the 21st century, just as oil powered industrial economies in the past.

* Both data and oil are valuable but require processing to be useful. Oil needs refining, and data needs organization and analysis to extract insights.

* Major tech companies like Facebook and Google are the big players in the data economy, similar to how oil companies dominated in the past.

* Other industries are also leveraging data to transform their operations and generate revenue.

* The analogy extends to the issue of control. Just like control over oil granted immense power, companies with vast amounts of valuable data hold significant influence.

* The passage concludes by mentioning potential risks associated with a few big players controlling personal data.

# Data Ownership

Data ownership refers to the rights and control a person, team, or organization has over specific data sets. This control can range from basic oversight to strict legal regulations. Here's a breakdown of the concept:

Accountability: Similar to assigning responsibility in projects, data ownership ensures someone is accountable for managing a data set.

Spectrum of Control: The level of control can vary. Data ownership might involve basic oversight or enforce strict rules regarding data access and usage.

Example: Data related to intellectual property (copyrights, trade secrets) will likely have strong ownership controls, including limitations on who can access it and how it can be used.

# Data Architecture

When designing the techincal needs of an organization to support its business strategy, this practice is known as Enterprise Architecture(EA). Using standards and established principles, organizations can analyze, design, plan and implement the right technology, policies and projects to support business goals.

A subset EA is a data architecture. In the same manner, which you can consider the holistic nature of EA in support of the organizations's strategy, data architecture is the manner in which data design and management decisions are being made to align with EA and in turn, with the business. Simplified, data architecture is the agreed blueprint for how data supports an organizations's functions and technologies.

*Every organization is a technology business*: in today's digital age, businesses rely heavily on technology to function effectively.

*Enterprise Architecture (EA)*: This practice involves designing the technical needs of an organization to supprt its overall business strategy. Think of it as a comprehensive roadmap for IT systems and infrastructure.

*Data Architecture as a subset of EA*: Data architecture focuses specifically on how data is designed, managed and aligned with the broader EA and business goals. It's essentially the blueprint for how data is used to support the organization's functions and technologies.

*Benefits of strong Data Architecture*: When combined with a high-quality EA, a well-defined data architecture fosters smoother operations and facilitates adaptation to changing internal or market conditions.

*Drawbacks of poor Data Architecture*: The absence or poor implementation of data architecture can hinder digital transformation efforts, lead to increased complexity, and raise the risk of failure.

At a minimum, data architecture considers and typically supports:

  * Ensuring data is available to those who need it and are approved to use it.

  * Reducing the complexity of accessing and utilizing data.

  * Creating and enforcing data protections to support organizational policies and obligations.

  * Adopting and agreeing to data standards.

  * Optimizing the flow and efficient use of data to eliminate bottlenecks and duplication.

Data architecture is a direct reflection of data governance. An established and functioning data architecture immediately signals that an organization values data, manages it as a critical business asset and has controls in place to ensure that it aligns with business needs. Data architecture is not exclusive realm of technologists but a cross-organizational responsibility. In most medium to large organizations, data must effeciently flow across business silos, such as sales and product development and serve many different audiences in multiple forms.


# The Lifecycle of Data

![image](https://github.com/MisterWest11/Data-Governance-Accelerator/assets/152319557/279f241f-aa5e-4a54-a053-bb9008fb1870)


1. *Creation*: This is the stage at which data comes into being. It may be manual or automated and get created internally or externally. Data is created all the time by a vast number of activities that include system inputs and outputs.

2. *Storage*: Once data is created and assuming you want it available for later use, it must be stored. It most likely will be contained and managed in a database. The database needs a home, too as a local hard drive, server or cloud services.

3. *Usage*: Hopefully you're capturing and storing data because you want to use it. Maybe not immediately, but at some point, perhaps for analysis. Data may need to be processed to be useful. That could include cleansing it of errors, transforming it to another format and securing access rights.

4. *Archival*: In this stage, you identify data that is not currently being used and moved it to a long-term storage system out of your production environment. If it's needed at some point in the future, it can be retrieved and utilized it.

5. *Destruction*: Despite a desire by some to keep everything forever, there is a logical point where destruction makes sense or is required by regulation or policy. Data destruction involves making data inaccessible and unreadable. It can include the physical destruction of a device such as a hard drive.

# Understanding the Impact of Big Data

* Data: Not a New Concept: The concept of data collection and storage has existed for millennia. Examples include Roman ledgers used for record-keeping.

* The Cold War and Technological Advancements: The Cold War, a period of geopolitical tension between the US and USSR, fueled significant advancements in technology, including:

  * Microprocessors: These tiny chips became the backbone of modern computers.
    
  * Classical Computing: The foundation for the computers we use today.
 
* Space Race as a Catalyst: The competition between the US and USSR to reach space further accelerated innovation in:

  * Computing: More powerful computers were needed for complex calculations and simulations.
 
  * Telecommunications: Improved communication was crucial for space missions and international collaboration.


The U.S. Census and the Need for Data Processing: The slow and laborious task of manually processing the 1880 census data (taking nearly eight years!) highlighted the need for a faster and more efficient method. This is considered a driving force behind the birth of data processing.

The Tabulating Machine Company: Founded in 1896, this company addressed the census challenge by developing a system that used punched cards to represent data. These cards could be "read" by a machine to automate tabulation.

Punched Cards and Early Data Storage: The punched card system can be seen as an early form of data storage. Holes in the cards represented specific data points, allowing the machine to process information efficiently.

The Tabulating Machine Company Becomes IBM: This company, later renamed International Business Machines (IBM), became a pioneer in data processing technology.

Data Processing Spreads: IBM's innovations and advancements in data processing technology infiltrated various aspects of life, including offices, military, academia, and factories, leading to increased automation.

The Information Age and Big Data: These developments paved the way for the Information Age, where data became the new "raw material" for productivity and innovation. Connectivity facilitated the flow of information across devices and locations.

The Rise of Big Data: With the exponential growth of data creation and storage, traditional methods struggled to manage it. This phenomenon led to the coining of the term "Big Data" to describe the vast and complex nature of data in the modern era.

In essence, the U.S. Census data processing challenge sparked innovations that revolutionized data management and fueled the Information Age and the emergence of Big Data.

# Defining Big Data

Big data is structured and unstructured data that is so massive and complex in scale, that it's difficult and often impossible to process via traditional data management techniques.

One way to define and characterize big data is through these 5 V's:

 * *Volume*: The sheer scale of data being produced is unprecedented and requires new tools, skills, and processes.

 * *Variety*: There are already a lot of legacy file formats, such as CSV & MP3 and new innovations, new formats are emerging all the time. This requires different methods of handling from analysis to security.

 * *Velocity*: With so many collection points, digital interfaces and ubiquitous connectivity, data is being created and moved at increasing speed. Consider that in 2021, Instagram users created, uploaded and share 65 000 pictures a minute.

 * *Variability*: The fact that the creation and flow of data are unpredictable.

 * *Veracity*: The quality, including accuracy and truthfulness, large volume of disparate sets of data, can differ considerably, causing challenges to data management.

**Drivers of Big Data**

 * Early Signs of Big Data: Even before the explosion of smartphones, apps, and the Internet of Things, Big Data was already a growing concern. This emphasizes that the roots of Big Data lie deeper than just recent technological advancements.

 * The Cambrian Explosion Analogy: The passage uses the term "Cambrian explosion" to describe the rapid surge in data volume. This analogy refers to a period in Earth's history marked by the sudden diversification of life forms.

 * Growth in Data Volume: The data volume has been steadily increasing, reaching the zettabyte range(a vast amount of data) by the third decade of the 21st century.

 * Predictions for Continued Growth: The passage predicts a continuous rise in data generation, with estimates reaching 180 zettabytes by 2025.

 * Shifting Perspective: The passage highlights how our perception of data volume changes over time. Just as 32GB of storage seemed like a lot in 2015, future generations might find today's zettabyte figures insignificant due to the ever-increasing data deluge.

![image](https://github.com/MisterWest11/Data-Governance-Accelerator/assets/152319557/872aec76-35ca-4e1f-943b-ade2e113b386)

**Consequences of Big Data**

Data Quality: A significant portion (up to 80%) of Big Data is unstructured, making it difficult to analyze and utilize. Many organizations struggle with effectively managing unstructured data.

Data Duplication and Errors: Data redundancy and inaccuracies can be prevalent within Big Data sets, requiring additional effort for cleaning and processing.

Accessibility Issues: Big Data may not be readily accessible due to various factors, hindering its potential value.

Opportunities of Big Data:

Value Creation: Despite the challenges, Big Data offers immense value for organizations across various sectors.

Innovation Driver: Big Data is a catalyst for innovation, driving advancements in areas like self-driving cars and supply chain optimization.

Enhanced Marketing: Big Data, combined with hyperconnectivity, empowers marketers to gain granular insights into the market and target specific audiences for maximized results.

**What about Small Data**

Small Data's Value: Contrary to the emphasis on Big Data, small amounts of data can still be valuable and insightful.

Small Data in Everyday Operations: Spreadsheets, surveys, and personal to-do lists are all examples of small data that play a crucial role in day-to-day decisions and business activities.

Small Data and Big Questions: Even significant issues can be addressed through analysis of small data sets.

Small Data - A Part of Big Data: Big Data often gains meaning and usability when broken down into smaller, more manageable chunks. In essence, small data can be seen as a building block for understanding Big Data.
